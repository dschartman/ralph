{"id": "ralph-1a8097", "title": "Planner: Add investigation task guidance", "description": "Planner prompt should include guidance for creating investigation/spike tasks when root cause is unclear. Pattern: When same criterion fails 2+ iterations with similar errors, don't assign another fix - assign investigation first. Investigation is valid Executor work. After root cause is known, Planner can decide: fix or deprioritize. This is standard agile practice.", "status": "closed", "priority": 2, "created_at": "2026-01-18T16:03:32.227868Z", "updated_at": "2026-01-19T15:15:35.316275Z", "closed_at": "2026-01-19T15:15:35.316275Z", "dependencies": [{"depends_on_id": "ralph-1c9p4g", "type": "parent"}], "comments": [{"content": "## Code Analysis\n\n**File to modify:**\n- `src/ralph2/agents/planner.py` - Add investigation guidance to system prompt\n\n**Current state:**\n- Planner prompt has \"Task Decomposition\" section (lines 48-55) but no mention of investigation tasks\n- No guidance on when to create investigation vs fix tasks\n\n**Implementation steps:**\n\n1. Add new section to `PLANNER_SYSTEM_PROMPT` after \"Task Decomposition\" (~line 55):\n\n```python\n## Investigation Tasks\n\nWhen root cause is unclear, assign investigation before fixes:\n\n**Pattern to recognize:**\n- Same criterion fails 2+ iterations with similar errors\n- Executor reports \"Blocked\" or \"Uncertain\" without clear path forward\n- Verifier says \"unverifiable\" but no clear reason why\n\n**When this happens:**\n1. Do NOT assign another fix attempt\n2. Create an investigation task: \"Investigate: why does X fail?\"\n3. Investigation is valid Executor work (see below)\n4. After root cause is known, decide: fix or deprioritize\n\n**Investigation task template:**\n- Title: \"Investigate: <symptom>\"\n- Description: \"Root cause unknown. Previous attempts: <list>. Goal: identify why <X> happens and recommend fix approach.\"\n\nThis is standard agile practice: spikes before implementation when requirements are unclear.\n```\n\n**Test:** Create a spec with a tricky test case, let it fail 2 iterations, verify Planner creates investigation task on iteration 3.", "source": "engineer", "created_at": "2026-01-19T15:09:10.376970Z"}]}
{"id": "ralph-1c9p4g", "title": "Agent role clarity and iteration awareness", "description": "Refine agent responsibilities and add iteration history visibility to improve multi-iteration problem solving. Root issue: Planner couldn't recognize recurring failures because it only sees last iteration, and Executors were declaring 'Completed' without verification.", "status": "closed", "priority": 2, "created_at": "2026-01-18T16:03:22.345586Z", "updated_at": "2026-01-19T15:15:52.928371Z", "closed_at": "2026-01-19T15:15:52.928371Z", "dependencies": [], "comments": [{"content": "## Handoff Summary (2026-01-19)\n\n### Context\nDiscussed conceptually whether these should be structural changes (multi-step orchestrator) or prompt fixes. Conclusion: **prompt fixes are the right approach** for now. The multi-step orchestrator idea was deferred - see ralph-i5rcqp for future work on capturing step patterns.\n\n### Approach\nAll remaining work is prompt/data changes to existing agents. No architectural changes needed.\n\n### Children Status\n- **ralph-dyjo23** (Planner iteration history) - DATA + PROMPT - detailed implementation in comments\n- **ralph-1a8097** (Planner investigation guidance) - PROMPT only - detailed implementation in comments  \n- **ralph-7ctr57** (Executor completion clarity) - PROMPT only - detailed implementation in comments\n- **ralph-usxwlr** (Verifier scope) - CLOSED - already implemented in feature/verifier-assessor-refactor\n- **ralph-41jjlp** (Executor investigation work) - PROMPT only - detailed implementation in comments\n\n### Key Files\n- `src/ralph2/agents/planner.py` - Planner system prompt and run_planner()\n- `src/ralph2/agents/executor.py` - Executor system prompt\n- `src/ralph2/runner.py` - Orchestration, needs to build iteration history\n\n### Testing\nEach child issue has a test approach in its comments. Run `uv run pytest tests/ -v` after changes.\n\n### Related\n- ralph-i5rcqp: Future work on capturing agent step patterns for meta-analysis", "source": "engineer", "created_at": "2026-01-19T15:11:15.113788Z"}, {"content": "## Implementation Complete (2026-01-19)\n\nAll children implemented and tested:\n\n### Planner Changes (planner.py)\n- Added **Pattern Recognition** section to system prompt - helps identify recurring failures\n- Added **Investigation Tasks** section to system prompt - guides when to create investigation vs fix tasks\n- Added `iteration_history` parameter to `run_planner()` function\n- Updated prompt building to include iteration history for pattern recognition\n\n### Executor Changes (executor.py)  \n- Added **CRITICAL: What 'Completed' Means** section - clarifies Completed = tests run and pass\n- Added **Investigation Tasks** section - explains investigation work is valid and how to do it\n- Updated **Valid Exit Conditions** to reinforce: Blocked is preferred over Completed without verification\n\n### Runner Changes (runner.py)\n- Added `_build_iteration_history()` method to build summary of previous iterations\n- Updated `_run_planner_with_retry()` to build and pass iteration history\n\n### Tests\n- All 340 ralph2 tests pass\n- Updated 4 tests in `test_planner_error_handling.py` to mock the new iteration history building\n\n### Files Modified\n- `src/ralph2/agents/planner.py` (system prompt + function signature + prompt building)\n- `src/ralph2/agents/executor.py` (system prompt)\n- `src/ralph2/runner.py` (iteration history building)\n- `tests/ralph2/test_planner_error_handling.py` (test fixes)", "source": "executor", "created_at": "2026-01-19T15:15:49.910887Z"}]}
{"id": "ralph-41jjlp", "title": "Executor: Clarify investigation is valid work", "description": "Executor prompt should clarify that investigation/research tasks are valid work. When assigned 'investigate root cause of X', the Executor should: run the failing code, capture full output/traceback, analyze the error chain, identify actual root cause, report findings. This is distinct from 'fix X' tasks. Executors do thorough work - whether that's writing code or researching problems.", "status": "closed", "priority": 2, "created_at": "2026-01-18T16:03:48.002781Z", "updated_at": "2026-01-19T15:15:35.317045Z", "closed_at": "2026-01-19T15:15:35.317045Z", "dependencies": [{"depends_on_id": "ralph-1c9p4g", "type": "parent"}], "comments": [{"content": "## Code Analysis\n\n**File to modify:**\n- `src/ralph2/agents/executor.py` - Add investigation task guidance to system prompt\n\n**Current state:**\n- EXECUTOR_SYSTEM_PROMPT says \"Do ONLY that work\u2014nothing else\" (line 36)\n- No mention that investigation/research tasks are valid work\n- Executor might be confused when assigned \"investigate root cause of X\"\n\n**Implementation steps:**\n\n1. Add new section to `EXECUTOR_SYSTEM_PROMPT` after \"Test-Driven Development\" section (~line 91):\n\n```python\n## Investigation Tasks\n\nInvestigation/research tasks are valid work. When assigned \"Investigate: X\":\n\n**Your deliverable is information, not code:**\n1. Reproduce the issue (run failing code, capture full output/traceback)\n2. Analyze the error chain (what triggers what?)\n3. Identify the actual root cause (not symptoms)\n4. Document findings in a Trace comment\n5. Recommend fix approach (but don't implement unless assigned)\n\n**Investigation output structure:**\n```\n## Findings for: <issue>\n\n**Reproduction:**\n- Command: `<what you ran>`\n- Error: `<exact error message>`\n\n**Root Cause:**\n<explanation of why this happens>\n\n**Recommended Fix:**\n<approach to fix, not full implementation>\n```\n\n**Status after investigation:**\n- Completed = You identified root cause and documented findings\n- Blocked = You cannot reproduce or need external resources\n- Uncertain = Multiple possible causes, need guidance on which to pursue\n```\n\n**Test:** Create an investigation task in Trace, assign to Executor, verify it produces findings rather than trying to write code.", "source": "engineer", "created_at": "2026-01-19T15:09:12.272837Z"}]}
{"id": "ralph-7ctr57", "title": "Executor: Clarify completion means verified", "description": "Executor prompt should clarify: work is 'Completed' only when verified by running tests. If tests can't be run (missing credentials, too slow, etc.), status is 'Blocked' not 'Completed.' Executors should not punt verification to the Verifier. The pattern 'I made a fix but needs verification' should not result in 'Completed' status.", "status": "closed", "priority": 2, "created_at": "2026-01-18T16:03:37.308055Z", "updated_at": "2026-01-19T15:15:35.316708Z", "closed_at": "2026-01-19T15:15:35.316708Z", "dependencies": [{"depends_on_id": "ralph-1c9p4g", "type": "parent"}], "comments": [{"content": "## Code Analysis\n\n**File to modify:**\n- `src/ralph2/agents/executor.py` - Strengthen completion requirements in system prompt\n\n**Current state:**\n- EXECUTOR_SYSTEM_PROMPT line 150-154 defines exit conditions:\n  - \"**Completed**: Work finished, tests pass, changes committed, traces updated\"\n- But this isn't strongly enforced - executors sometimes claim Completed without running tests\n- Lines 99-114 discuss verification boundaries but focus on capability vs behavior distinction\n\n**Implementation steps:**\n\n1. Add explicit section to `EXECUTOR_SYSTEM_PROMPT` after \"Test-Driven Development\" (~line 91):\n\n```python\n## CRITICAL: What \"Completed\" Means\n\n**Completed = Verified by Tests**\n\nYou may ONLY report status \"Completed\" when:\n1. You have written tests for the work (if applicable)\n2. You have RUN the tests (`uv run pytest` or equivalent)\n3. The tests PASS\n\n**If you cannot run tests:**\n- Tests require external resources \u2192 Status: **Blocked**, not Completed\n- Tests are too slow \u2192 Status: **Blocked**, not Completed  \n- No test framework exists \u2192 Status: **Blocked**, not Completed\n\n**The pattern \"I made a fix, needs verification\" = Blocked, NOT Completed**\n\nNever punt verification to the Verifier. The Verifier checks spec satisfaction, not your work quality. You own verification of your own work.\n```\n\n2. Update the \"Valid Exit Conditions\" section (lines 149-154) to reinforce:\n\n```python\n## Valid Exit Conditions\n\n- **Completed**: Tests written, tests run, tests PASS, changes committed\n- **Blocked**: Can't proceed OR can't verify (missing deps, external resources)\n- **Uncertain**: Not sure if approach is correct, need guidance\n```\n\n**Test:** Assign a code task, verify Executor runs tests before claiming Completed.", "source": "engineer", "created_at": "2026-01-19T15:09:10.714046Z"}]}
{"id": "ralph-dyjo23", "title": "Planner: Add iteration history visibility", "description": "Planner currently only sees last iteration's feedback. Should see summary of ALL previous iterations to recognize patterns like 'same test has failed 3 times with similar errors.' This enables the Planner to say 'stop trying fixes, investigate root cause first.' Implementation: Pass iteration history (number, intent, outcome, executor summary) to Planner prompt.", "status": "closed", "priority": 2, "created_at": "2026-01-18T16:03:27.564996Z", "updated_at": "2026-01-19T15:15:35.315472Z", "closed_at": "2026-01-19T15:15:35.315472Z", "dependencies": [{"depends_on_id": "ralph-1c9p4g", "type": "parent"}], "comments": [{"content": "## Code Analysis\n\n**Files to modify:**\n- `src/ralph2/runner.py` - Build iteration history before calling planner\n- `src/ralph2/agents/planner.py` - Add iteration_history parameter and update prompt\n\n**Current state:**\n- Planner only receives last iteration's feedback via `last_executor_summary`, `last_verifier_assessment`, `last_specialist_feedback` (runner.py:820-828)\n- No visibility into patterns across multiple iterations\n\n**Implementation steps:**\n\n1. In `runner.py`, add helper method to build iteration history:\n```python\ndef _build_iteration_history(self, run_id: str, current_iteration: int) -> list[dict]:\n    \"\"\"Build summary of previous iterations for pattern recognition.\"\"\"\n    iterations = self.db.list_iterations(run_id)\n    history = []\n    for it in iterations:\n        if it.number < current_iteration:\n            history.append({\n                \"number\": it.number,\n                \"intent\": it.intent,\n                \"outcome\": it.outcome,\n                # Get executor summary from agent outputs\n            })\n    return history\n```\n\n2. In `run_planner()` signature (planner.py:202), add parameter:\n```python\niteration_history: Optional[list[dict]] = None,\n```\n\n3. In planner prompt building (planner.py:229-316), add iteration history section:\n```python\nif iteration_history:\n    prompt_parts.append(\"# Iteration History\")\n    prompt_parts.append(\"\")\n    for h in iteration_history:\n        prompt_parts.append(f\"## Iteration {h['number']}\")\n        prompt_parts.append(f\"Intent: {h['intent']}\")\n        prompt_parts.append(f\"Outcome: {h['outcome']}\")\n        prompt_parts.append(\"\")\n```\n\n4. In `PLANNER_SYSTEM_PROMPT`, add guidance:\n```\n## Pattern Recognition\n\nReview iteration history to identify recurring patterns:\n- Same test failing multiple iterations \u2192 investigate root cause before more fixes\n- Same criterion stuck as unverifiable \u2192 may need different approach\n- Repeated \"Blocked\" from executors \u2192 investigate blocker systematically\n```\n\n**Test:** Run ralph2 for 3+ iterations and verify Planner prompt includes history.", "source": "engineer", "created_at": "2026-01-19T15:09:07.922298Z"}]}
{"id": "ralph-i5rcqp", "title": "Capture agent step patterns for meta-analysis", "description": "Enhance logging to capture structured data about what agents do when they succeed, enabling identification of meta-patterns that could inform future architecture decisions.\n\n## Context\nCurrent 3-agent structure (Planner/Executor/Verifier) works, but we want to understand:\n- What steps do agents consistently take when runs converge successfully?\n- Are there repeated sequences/patterns across successful runs?\n- What's the right granularity for future iterations of Ralph?\n\n## The insight\nRalph's principle (fresh context, focused task, loop until convergence) could potentially apply at finer granularity than 3 agents. But we need data to know where the natural boundaries are.\n\n## What to capture\n1. **Step sequences** - What each agent actually did, in order\n2. **Outcomes** - Did iteration make progress? Did run converge? How many iterations?\n3. **Correlation** - Which patterns appear in successful vs unsuccessful runs\n\n## Potential approaches\n- Add structured `steps_taken` field to agent outputs with defined vocabulary\n- Improve existing JSONL logging to be more analyzable\n- Build tooling to query patterns across runs\n\n## Goal\nCollect enough data to answer: \"Here's everything agents do consistently when we get a good result.\"", "status": "open", "priority": 2, "created_at": "2026-01-19T15:03:27.547254Z", "updated_at": "2026-01-19T15:03:27.547254Z", "closed_at": null, "dependencies": [], "comments": []}
{"id": "ralph-usxwlr", "title": "Verifier: Narrow scope to spec + test adequacy", "description": "Verifier's job is narrowly: (1) Is spec satisfied? (2) Are tests adequate to prove it? Verifier should NOT do root cause analysis on failures. Should NOT research why tests break. CAN say 'these tests are not good enough to have confidence spec is satisfied.' Focus: spec criteria + test quality + thumbs up/down. Remove efficiency notes from Verifier output - that's Executor's job.", "status": "closed", "priority": 2, "created_at": "2026-01-18T16:03:42.388358Z", "updated_at": "2026-01-19T15:10:32.825802Z", "closed_at": "2026-01-19T15:10:32.825802Z", "dependencies": [{"depends_on_id": "ralph-1c9p4g", "type": "parent"}], "comments": [{"content": "## Code Analysis\n\n**File to modify:**\n- `src/ralph2/agents/verifier.py` - Review and potentially remove efficiency notes\n\n**Current state:**\n- Verifier is already assessor-only (good) - see lines 15-20: \"You are an ASSESSOR, not a decision-maker\"\n- Verifier correctly does NOT decide CONTINUE/DONE/STUCK (lines 74-78)\n- BUT Verifier still has efficiency_notes section (lines 82-96) and outputs them\n\n**Issue description says:** \"Remove efficiency notes from Verifier output - that's Executor's job\"\n\n**Decision point:** This is debatable. Current efficiency notes are verification-specific:\n- \"Run `uv run pytest -v` to check all test criteria at once\"\n- \"CLI criteria can be verified with `--help` flags\"\n\nThese ARE useful for future Verifiers. But the issue says Executor owns efficiency notes.\n\n**If we proceed with removal:**\n\n1. Remove efficiency notes section from `VERIFIER_SYSTEM_PROMPT` (lines 82-96)\n\n2. Remove `efficiency_notes` field from `VerifierResult` model in `models.py` (line 120-123)\n\n3. Update `run_verifier()` to not include efficiency_notes in assessment output\n\n**Alternative (if we keep Verifier efficiency notes):**\n- Close this issue as \"won't fix\" with rationale\n- Verifier efficiency notes are about verification shortcuts, distinct from Executor efficiency notes about implementation shortcuts\n\n**Recommendation:** Ask product owner whether Verifier efficiency notes should be kept. They serve a different purpose than Executor notes.\n\n**What's definitely correct (already done):**\n- Verifier is assessor-only \u2713\n- Verifier does not decide CONTINUE/DONE/STUCK \u2713\n- Verifier does not recommend next steps \u2713", "source": "engineer", "created_at": "2026-01-19T15:09:11.925950Z"}, {"content": "## Decision: Keep Verifier efficiency notes\n\nEfficiency notes stay in Verifier. The purpose is to capture learnings from different perspectives:\n- Executor: implementation shortcuts (\"config is in settings.py\")\n- Verifier: verification shortcuts (\"run pytest -v to check all criteria\")\n\nBoth feed into project memory and help future iterations.\n\n## Revised Scope\n\nThe core requirement (Verifier is assessor-only) is **already implemented**:\n- \u2713 Verifier does NOT decide CONTINUE/DONE/STUCK (that's Planner's job)\n- \u2713 Verifier does NOT recommend next steps\n- \u2713 Verifier ONLY reports spec satisfaction status\n\n**No code changes needed.** This issue can be closed.\n\nVerify by reading `src/ralph2/agents/verifier.py` lines 15-20 and 74-78.", "source": "engineer", "created_at": "2026-01-19T15:10:28.261643Z"}]}
