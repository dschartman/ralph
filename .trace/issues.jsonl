{"id": "ralph-127ogi", "title": "Project scaffolding: soda package structure", "description": "Create the foundational project structure for the soda package.\n\n## What to create\n1. src/soda/ directory with __init__.py\n2. src/soda/agents/ directory with __init__.py  \n3. src/soda/outputs/ directory with __init__.py\n4. Update pyproject.toml to include soda package\n5. Create basic type definitions (src/soda/types.py) - placeholder Pydantic models\n\n## Technical constraints\n- Use Python 3.11+ features\n- Use Pydantic v2 for model definitions\n- Structure should mirror src/ralph2/ patterns where appropriate\n\n## Verification\n- `import soda` should work\n- `import soda.agents` should work\n- pyproject.toml has soda in packages", "status": "closed", "priority": 2, "created_at": "2026-01-20T17:47:48.829891Z", "updated_at": "2026-01-20T17:50:35.463205Z", "closed_at": "2026-01-20T17:50:35.463205Z", "dependencies": [{"depends_on_id": "ralph-1tbdk4", "type": "parent"}], "comments": [{"content": "Starting work on project scaffolding for soda package", "source": "executor", "created_at": "2026-01-20T17:48:47.821949Z"}, {"content": "Completed project scaffolding:\n- Created src/soda/ directory structure with __init__.py\n- Created src/soda/agents/__init__.py for agent modules\n- Created src/soda/outputs/__init__.py for output capture\n- Created src/soda/types.py with Pydantic models (AgentConfig, StructuredOutput, AgentInvocation, Message, Conversation, AgentError, ValidationError)\n- Updated pyproject.toml to include soda script entry point\n- Created tests/soda/test_scaffolding.py with 16 tests - all passing\n\nVerified:\n- import soda works\n- import soda.agents works\n- import soda.outputs works\n- All type definitions work with Pydantic v2", "source": "executor", "created_at": "2026-01-20T17:50:22.040597Z"}]}
{"id": "ralph-148trj", "title": "Implement error handling with retry logic", "description": "Create error handling infrastructure that distinguishes transient vs fatal errors.\n\n## Requirements\n- Transient errors: rate limit, timeout, connection error, 5xx \u2192 retry with exponential backoff (max 3 attempts)\n- Fatal errors: invalid API key, 401, 403, permission denied \u2192 halt immediately\n- Unknown errors \u2192 treat as transient (retry)\n- After max retries exhausted \u2192 surface error with full context\n\n## Interface\n```python\nclass RetryHandler:\n    def execute_with_retry(self, func: Callable, max_attempts: int = 3) -> T:\n        \"\"\"Execute function with exponential backoff retry on transient errors.\"\"\"\n        \ndef is_transient_error(error: Exception) -> bool:\n    \"\"\"Classify error as transient (retry) or fatal (halt).\"\"\"\n```\n\n## Verification\n- Transient errors trigger retries up to max_attempts\n- Fatal errors halt immediately without retry\n- Backoff increases exponentially between retries\n- Error context preserved in final exception", "status": "closed", "priority": 2, "created_at": "2026-01-20T17:48:14.084944Z", "updated_at": "2026-01-20T17:56:46.997978Z", "closed_at": "2026-01-20T17:56:46.997978Z", "dependencies": [{"depends_on_id": "ralph-1tbdk4", "type": "parent"}], "comments": [{"content": "Starting work on error handling with retry logic implementation", "source": "executor", "created_at": "2026-01-20T17:54:42.504716Z"}, {"content": "Completed implementation of error handling with retry logic.\n\nCreated src/soda/errors.py with:\n- TransientError, FatalError, MaxRetriesExhaustedError exception classes\n- is_transient_error() function for error classification\n- RetryHandler class with execute_with_retry() method implementing exponential backoff\n\nTest coverage (29 tests):\n- TestIsTransientError: 10 tests verifying error classification\n- TestTransientError/FatalError/MaxRetriesExhaustedError: 6 tests for exception classes\n- TestRetryHandler: 13 tests verifying retry behavior, backoff, and error context\n\nAll tests passing (78/78 in soda test suite).", "source": "executor", "created_at": "2026-01-20T17:56:31.871785Z"}]}
{"id": "ralph-1528jp", "title": "src/ralph2/agents/verifier.py:198-239: Assessment string building logic is duplicated. Lines 198-211 build assessment for Trace comment, and lines 228-239 build nearly identical assessment for runner consumption. Both loops iterate over criteria_status with similar formatting.. Code duplication increases maintenance burden. If the assessment format needs to change, two places need to be updated which is error-prone. The current code already has a slight difference in format (line 203 uses `[{symbol}]` while line 233 uses `{symbol}`) which may or may not be intentional.. Extract a `_build_assessment_string(result: VerifierResult, format_style: str = 'standard') -> str` helper function that can be parameterized for the minor formatting differences, or use the same format for both use cases.", "description": "Feedback from code_reviewer:\n\nsrc/ralph2/agents/verifier.py:198-239: Assessment string building logic is duplicated. Lines 198-211 build assessment for Trace comment, and lines 228-239 build nearly identical assessment for runner consumption. Both loops iterate over criteria_status with similar formatting.. Code duplication increases maintenance burden. If the assessment format needs to change, two places need to be updated which is error-prone. The current code already has a slight difference in format (line 203 uses `[{symbol}]` while line 233 uses `{symbol}`) which may or may not be intentional.. Extract a `_build_assessment_string(result: VerifierResult, format_style: str = 'standard') -> str` helper function that can be parameterized for the minor formatting differences, or use the same format for both use cases.", "status": "open", "priority": 2, "created_at": "2026-01-20T18:00:26.165700Z", "updated_at": "2026-01-20T18:00:26.165700Z", "closed_at": null, "dependencies": [{"depends_on_id": "ralph-1pqntk", "type": "parent"}], "comments": []}
{"id": "ralph-1620s1", "title": "Implement bookended agent pattern", "description": "Create the bookended agent pattern - setup prompts, main work, wrap-up prompts.\n\n## Requirements\n- Execute setup prompts before main work prompt\n- Execute main work prompt\n- Execute wrap-up prompts in same context after main work\n- Capture full conversation (setup + work + wrap-up) to JSONL\n\n## Interface\n```python\nclass BookendedAgent:\n    def invoke(\n        self,\n        setup_prompts: List[str],\n        work_prompt: str,\n        wrapup_prompts: List[str],\n        output_schema: Optional[Type[BaseModel]] = None\n    ) -> BookendedResult:\n        \"\"\"Execute bookended pattern, return all results.\"\"\"\n```\n\n## Dependencies\n- Output capture module  \n- Walked agent (for context persistence)\n- Structured output validation (optional for work result)\n- Error handling with retry\n\n## Verification\n- Setup prompts execute first in order\n- Work prompt executes with setup context\n- Wrap-up prompts execute with full context\n- All captured to single JSONL file", "status": "open", "priority": 2, "created_at": "2026-01-20T17:48:16.889754Z", "updated_at": "2026-01-20T17:48:16.889754Z", "closed_at": null, "dependencies": [{"depends_on_id": "ralph-1tbdk4", "type": "parent"}], "comments": []}
{"id": "ralph-16h8y0", "title": "Implement structured output validation", "description": "Create Pydantic-based structured output validation for agent responses.\n\n## Requirements\n- Parse agent output against expected Pydantic schema\n- Return typed object on successful validation\n- Raise detailed validation error on schema mismatch\n- System halts on validation failure (no retry at agent level)\n\n## Interface\n```python\nclass StructuredOutputValidator:\n    def validate(self, raw_output: str, schema: Type[BaseModel]) -> T:\n        \"\"\"Validate raw output against schema, return typed object or raise.\"\"\"\n```\n\n## Verification\n- Valid output returns typed Pydantic model\n- Invalid output raises ValidationError with field details\n- Validation errors include what was expected vs received", "status": "closed", "priority": 2, "created_at": "2026-01-20T17:48:14.656197Z", "updated_at": "2026-01-20T17:56:35.010322Z", "closed_at": "2026-01-20T17:56:35.010322Z", "dependencies": [{"depends_on_id": "ralph-1tbdk4", "type": "parent"}], "comments": [{"content": "Starting work on structured output validation. First, exploring existing codebase structure to understand where this should be placed.", "source": "executor", "created_at": "2026-01-20T17:54:45.055597Z"}, {"content": "Implementation complete. Created src/soda/validation.py with:\n- StructuredOutputValidator class with validate(raw_output, schema) method\n- StructuredOutputValidationError exception with detailed error info\n- Proper extraction of field paths for nested schemas\n- Handling of JSON parse errors\n\nAll 14 tests in test_structured_output_validator.py pass. Tests cover:\n- Valid JSON returns typed Pydantic model\n- Missing required fields raise validation errors\n- Wrong field types raise validation errors\n- Invalid JSON raises validation errors\n- Nested schemas validated correctly\n- Error includes field path and received value\n- Optional fields and defaults work correctly", "source": "executor", "created_at": "2026-01-20T17:56:17.873738Z"}]}
{"id": "ralph-1a8097", "title": "Planner: Add investigation task guidance", "description": "Planner prompt should include guidance for creating investigation/spike tasks when root cause is unclear. Pattern: When same criterion fails 2+ iterations with similar errors, don't assign another fix - assign investigation first. Investigation is valid Executor work. After root cause is known, Planner can decide: fix or deprioritize. This is standard agile practice.", "status": "closed", "priority": 2, "created_at": "2026-01-18T16:03:32.227868Z", "updated_at": "2026-01-19T15:15:35.316275Z", "closed_at": "2026-01-19T15:15:35.316275Z", "dependencies": [{"depends_on_id": "ralph-1c9p4g", "type": "parent"}], "comments": [{"content": "## Code Analysis\n\n**File to modify:**\n- `src/ralph2/agents/planner.py` - Add investigation guidance to system prompt\n\n**Current state:**\n- Planner prompt has \"Task Decomposition\" section (lines 48-55) but no mention of investigation tasks\n- No guidance on when to create investigation vs fix tasks\n\n**Implementation steps:**\n\n1. Add new section to `PLANNER_SYSTEM_PROMPT` after \"Task Decomposition\" (~line 55):\n\n```python\n## Investigation Tasks\n\nWhen root cause is unclear, assign investigation before fixes:\n\n**Pattern to recognize:**\n- Same criterion fails 2+ iterations with similar errors\n- Executor reports \"Blocked\" or \"Uncertain\" without clear path forward\n- Verifier says \"unverifiable\" but no clear reason why\n\n**When this happens:**\n1. Do NOT assign another fix attempt\n2. Create an investigation task: \"Investigate: why does X fail?\"\n3. Investigation is valid Executor work (see below)\n4. After root cause is known, decide: fix or deprioritize\n\n**Investigation task template:**\n- Title: \"Investigate: <symptom>\"\n- Description: \"Root cause unknown. Previous attempts: <list>. Goal: identify why <X> happens and recommend fix approach.\"\n\nThis is standard agile practice: spikes before implementation when requirements are unclear.\n```\n\n**Test:** Create a spec with a tricky test case, let it fail 2 iterations, verify Planner creates investigation task on iteration 3.", "source": "engineer", "created_at": "2026-01-19T15:09:10.376970Z"}]}
{"id": "ralph-1c9p4g", "title": "Agent role clarity and iteration awareness", "description": "Refine agent responsibilities and add iteration history visibility to improve multi-iteration problem solving. Root issue: Planner couldn't recognize recurring failures because it only sees last iteration, and Executors were declaring 'Completed' without verification.", "status": "closed", "priority": 2, "created_at": "2026-01-18T16:03:22.345586Z", "updated_at": "2026-01-19T15:15:52.928371Z", "closed_at": "2026-01-19T15:15:52.928371Z", "dependencies": [], "comments": [{"content": "## Handoff Summary (2026-01-19)\n\n### Context\nDiscussed conceptually whether these should be structural changes (multi-step orchestrator) or prompt fixes. Conclusion: **prompt fixes are the right approach** for now. The multi-step orchestrator idea was deferred - see ralph-i5rcqp for future work on capturing step patterns.\n\n### Approach\nAll remaining work is prompt/data changes to existing agents. No architectural changes needed.\n\n### Children Status\n- **ralph-dyjo23** (Planner iteration history) - DATA + PROMPT - detailed implementation in comments\n- **ralph-1a8097** (Planner investigation guidance) - PROMPT only - detailed implementation in comments  \n- **ralph-7ctr57** (Executor completion clarity) - PROMPT only - detailed implementation in comments\n- **ralph-usxwlr** (Verifier scope) - CLOSED - already implemented in feature/verifier-assessor-refactor\n- **ralph-41jjlp** (Executor investigation work) - PROMPT only - detailed implementation in comments\n\n### Key Files\n- `src/ralph2/agents/planner.py` - Planner system prompt and run_planner()\n- `src/ralph2/agents/executor.py` - Executor system prompt\n- `src/ralph2/runner.py` - Orchestration, needs to build iteration history\n\n### Testing\nEach child issue has a test approach in its comments. Run `uv run pytest tests/ -v` after changes.\n\n### Related\n- ralph-i5rcqp: Future work on capturing agent step patterns for meta-analysis", "source": "engineer", "created_at": "2026-01-19T15:11:15.113788Z"}, {"content": "## Implementation Complete (2026-01-19)\n\nAll children implemented and tested:\n\n### Planner Changes (planner.py)\n- Added **Pattern Recognition** section to system prompt - helps identify recurring failures\n- Added **Investigation Tasks** section to system prompt - guides when to create investigation vs fix tasks\n- Added `iteration_history` parameter to `run_planner()` function\n- Updated prompt building to include iteration history for pattern recognition\n\n### Executor Changes (executor.py)  \n- Added **CRITICAL: What 'Completed' Means** section - clarifies Completed = tests run and pass\n- Added **Investigation Tasks** section - explains investigation work is valid and how to do it\n- Updated **Valid Exit Conditions** to reinforce: Blocked is preferred over Completed without verification\n\n### Runner Changes (runner.py)\n- Added `_build_iteration_history()` method to build summary of previous iterations\n- Updated `_run_planner_with_retry()` to build and pass iteration history\n\n### Tests\n- All 340 ralph2 tests pass\n- Updated 4 tests in `test_planner_error_handling.py` to mock the new iteration history building\n\n### Files Modified\n- `src/ralph2/agents/planner.py` (system prompt + function signature + prompt building)\n- `src/ralph2/agents/executor.py` (system prompt)\n- `src/ralph2/runner.py` (iteration history building)\n- `tests/ralph2/test_planner_error_handling.py` (test fixes)", "source": "executor", "created_at": "2026-01-19T15:15:49.910887Z"}]}
{"id": "ralph-1ejs7m", "title": "Implement output capture module", "description": "Create the output capture module that saves agent outputs to JSONL files.\n\n## Requirements\n- Save raw output to outputs/ directory\n- Include timestamp, agent type, and prompt summary in each record\n- JSONL format (one JSON object per line, append-friendly)\n- Non-blocking: capture failures must not affect agent result return\n\n## Interface\n```python\nclass OutputCapture:\n    def capture(self, agent_type: str, prompt_summary: str, output: Any) -> None:\n        \"\"\"Capture agent output to JSONL file. Non-blocking, swallows errors.\"\"\"\n```\n\n## Verification\n- Calling capture() creates/appends to JSONL file\n- File contains timestamp, agent_type, prompt_summary\n- If capture fails, no exception is raised", "status": "closed", "priority": 2, "created_at": "2026-01-20T17:48:13.969795Z", "updated_at": "2026-01-20T17:56:30.819979Z", "closed_at": "2026-01-20T17:56:30.819979Z", "dependencies": [{"depends_on_id": "ralph-1tbdk4", "type": "parent"}], "comments": [{"content": "Starting work on output capture module. Exploring codebase structure.", "source": "executor", "created_at": "2026-01-20T17:54:44.125197Z"}, {"content": "Completed implementation:\n- Created OutputCapture class in src/soda/outputs/capture.py\n- Updated src/soda/outputs/__init__.py to export OutputCapture\n- Created comprehensive test suite (19 tests) in tests/soda/test_output_capture.py\n- All tests pass\n\nImplementation features:\n- Saves to outputs/ directory by default (configurable via output_dir parameter)\n- JSONL format with timestamp, agent_type, prompt_summary, and output fields\n- Non-blocking: swallows all errors (file, JSON, directory creation)\n- Date-based filenames (agent_outputs_YYYY-MM-DD.jsonl) for organization\n- Appends to existing files for multiple captures", "source": "executor", "created_at": "2026-01-20T17:56:27.873339Z"}]}
{"id": "ralph-1flgpo", "title": "src/ralph2/runner.py:40-103: Functions _extract_spec_title() and slugify_spec_title() lack test coverage and could be moved to a utility module. These utility functions handle spec parsing and branch name generation but are not independently testable. They're implementation details mixed with orchestration logic.. Move to a dedicated utils.py module with unit tests, or add tests that verify edge cases (empty specs, special characters, unicode).", "description": "Feedback from code_reviewer:\n\nsrc/ralph2/runner.py:40-103: Functions _extract_spec_title() and slugify_spec_title() lack test coverage and could be moved to a utility module. These utility functions handle spec parsing and branch name generation but are not independently testable. They're implementation details mixed with orchestration logic.. Move to a dedicated utils.py module with unit tests, or add tests that verify edge cases (empty specs, special characters, unicode).", "status": "open", "priority": 2, "created_at": "2026-01-20T17:53:26.836049Z", "updated_at": "2026-01-20T17:53:26.836049Z", "closed_at": null, "dependencies": [{"depends_on_id": "ralph-1pqntk", "type": "parent"}], "comments": []}
{"id": "ralph-1g8li1", "title": "Implement walked agent pattern", "description": "Create the walked agent pattern - maintains conversation context across multiple prompts.\n\n## Requirements\n- Start a conversation that persists context\n- Send subsequent prompts to same agent context\n- Capture each response in sequence\n- End conversation and capture full transcript to JSONL\n\n## Interface\n```python\nclass WalkedAgent:\n    def start(self) -> None:\n        \"\"\"Start a walked conversation.\"\"\"\n        \n    def send(self, prompt: str) -> str:\n        \"\"\"Send prompt to existing context, return response.\"\"\"\n        \n    def end(self) -> List[Message]:\n        \"\"\"End conversation, capture to JSONL, return full transcript.\"\"\"\n```\n\n## Dependencies\n- Output capture module\n- Error handling with retry\n\n## Verification\n- Context persists: agent remembers previous exchanges\n- Each response captured in sequence\n- Full conversation written to JSONL on end()", "status": "closed", "priority": 2, "created_at": "2026-01-20T17:48:16.217854Z", "updated_at": "2026-01-20T18:04:14.290875Z", "closed_at": "2026-01-20T18:04:14.290875Z", "dependencies": [{"depends_on_id": "ralph-1tbdk4", "type": "parent"}], "comments": [{"content": "Starting work on walked agent pattern. Exploring existing codebase structure.", "source": "executor", "created_at": "2026-01-20T18:01:58.600393Z"}, {"content": "Implementation complete. Created WalkedAgent class with start(), send(), and end() methods. All 20 tests pass. Context persists across sends, output captured to JSONL on end().", "source": "executor", "created_at": "2026-01-20T18:04:00.521609Z"}]}
{"id": "ralph-1hraa1", "title": "src/ralph2/feedback.py:77-109: Function parse_feedback_item() lacks validation for malformed priority markers. The regex r'\\[P([0-4])\\]\\s*(.*)' will silently ignore malformed markers like '[P5]' or '[P-1]', defaulting to P2. This could hide data quality issues in specialist feedback.. Add logging or validation for unexpected priority formats to make issues visible during development/debugging.", "description": "Feedback from code_reviewer:\n\nsrc/ralph2/feedback.py:77-109: Function parse_feedback_item() lacks validation for malformed priority markers. The regex r'\\[P([0-4])\\]\\s*(.*)' will silently ignore malformed markers like '[P5]' or '[P-1]', defaulting to P2. This could hide data quality issues in specialist feedback.. Add logging or validation for unexpected priority formats to make issues visible during development/debugging.", "status": "open", "priority": 3, "created_at": "2026-01-20T17:53:27.049307Z", "updated_at": "2026-01-20T17:53:27.049307Z", "closed_at": null, "dependencies": [{"depends_on_id": "ralph-1pqntk", "type": "parent"}], "comments": []}
{"id": "ralph-1npgah", "title": "src/ralph2/runner.py:776-800: Magic strings for error classification scattered across method _is_recoverable_error(). The recoverable and fatal error patterns are hardcoded string lists that make the error classification logic brittle and hard to maintain. Adding new error patterns requires modifying the method directly.. Extract error patterns to module-level constants (e.g., RECOVERABLE_ERROR_PATTERNS, FATAL_ERROR_PATTERNS) that can be easily extended and tested independently.", "description": "Feedback from code_reviewer:\n\nsrc/ralph2/runner.py:776-800: Magic strings for error classification scattered across method _is_recoverable_error(). The recoverable and fatal error patterns are hardcoded string lists that make the error classification logic brittle and hard to maintain. Adding new error patterns requires modifying the method directly.. Extract error patterns to module-level constants (e.g., RECOVERABLE_ERROR_PATTERNS, FATAL_ERROR_PATTERNS) that can be easily extended and tested independently.", "status": "open", "priority": 2, "created_at": "2026-01-20T17:53:26.779636Z", "updated_at": "2026-01-20T17:53:26.779636Z", "closed_at": null, "dependencies": [{"depends_on_id": "ralph-1pqntk", "type": "parent"}], "comments": []}
{"id": "ralph-1tbdk4", "title": "Soda M1: Agent Infrastructure", "description": "# Soda Milestone 1: Agent Infrastructure\n\nFoundation layer for Soda that can invoke Claude agents in different patterns (narrow, walked, bookended) and handle their structured outputs reliably.\n\n## Acceptance Criteria\n\n### Narrow Agent Pattern\n- WHEN a narrow agent is invoked with a prompt and expected output schema, THEN it returns structured output matching the schema\n- WHEN a narrow agent is invoked with a tool allowlist, THEN the agent only has access to those tools\n- WHEN a narrow agent completes, THEN its full conversation is captured to a JSONL file\n\n### Walked Agent Pattern\n- WHEN a walked conversation is started, THEN subsequent prompts are sent to the same agent context\n- WHEN multiple prompts are sent to a walked agent, THEN each response is captured in sequence\n- WHEN a walked conversation is ended, THEN the full conversation is captured to a JSONL file\n\n### Bookended Agent Pattern\n- WHEN a bookended agent is invoked, THEN setup prompts execute before the main work prompt\n- WHEN main work completes, THEN wrap-up prompts execute in the same context\n- WHEN wrap-up completes, THEN the full conversation (setup + work + wrap-up) is captured\n\n### Structured Output\n- WHEN an agent returns output matching the expected schema, THEN the output is parsed and returned as a typed object\n- WHEN an agent returns output not matching the schema, THEN a validation error is raised with details\n- WHEN schema validation fails, THEN the system halts (does not retry at agent level)\n\n### Error Handling\n- WHEN a transient error occurs (rate limit, timeout, connection error, 5xx), THEN the system retries with exponential backoff (max 3 attempts)\n- WHEN a fatal error occurs (invalid API key, 401, 403, permission denied), THEN the system halts immediately\n- WHEN max retries are exhausted, THEN the error is surfaced with full context\n- WHEN an unknown error occurs, THEN it is treated as transient (retry)\n\n### Output Capture\n- WHEN any agent invocation completes, THEN raw output is saved to outputs/ directory\n- WHEN output is captured, THEN it includes timestamp, agent type, and prompt summary\n- WHEN output capture fails, THEN the agent result is still returned (capture is non-blocking)\n\n## Technical Constraints\n- Python 3.11+, uv for package management\n- Claude Agent SDK for agent invocations\n- Pydantic for structured output validation\n- JSONL format for output files", "status": "open", "priority": 2, "created_at": "2026-01-20T17:47:41.179729Z", "updated_at": "2026-01-20T17:47:41.179729Z", "closed_at": null, "dependencies": [], "comments": []}
{"id": "ralph-41jjlp", "title": "Executor: Clarify investigation is valid work", "description": "Executor prompt should clarify that investigation/research tasks are valid work. When assigned 'investigate root cause of X', the Executor should: run the failing code, capture full output/traceback, analyze the error chain, identify actual root cause, report findings. This is distinct from 'fix X' tasks. Executors do thorough work - whether that's writing code or researching problems.", "status": "closed", "priority": 2, "created_at": "2026-01-18T16:03:48.002781Z", "updated_at": "2026-01-19T15:15:35.317045Z", "closed_at": "2026-01-19T15:15:35.317045Z", "dependencies": [{"depends_on_id": "ralph-1c9p4g", "type": "parent"}], "comments": [{"content": "## Code Analysis\n\n**File to modify:**\n- `src/ralph2/agents/executor.py` - Add investigation task guidance to system prompt\n\n**Current state:**\n- EXECUTOR_SYSTEM_PROMPT says \"Do ONLY that work\u2014nothing else\" (line 36)\n- No mention that investigation/research tasks are valid work\n- Executor might be confused when assigned \"investigate root cause of X\"\n\n**Implementation steps:**\n\n1. Add new section to `EXECUTOR_SYSTEM_PROMPT` after \"Test-Driven Development\" section (~line 91):\n\n```python\n## Investigation Tasks\n\nInvestigation/research tasks are valid work. When assigned \"Investigate: X\":\n\n**Your deliverable is information, not code:**\n1. Reproduce the issue (run failing code, capture full output/traceback)\n2. Analyze the error chain (what triggers what?)\n3. Identify the actual root cause (not symptoms)\n4. Document findings in a Trace comment\n5. Recommend fix approach (but don't implement unless assigned)\n\n**Investigation output structure:**\n```\n## Findings for: <issue>\n\n**Reproduction:**\n- Command: `<what you ran>`\n- Error: `<exact error message>`\n\n**Root Cause:**\n<explanation of why this happens>\n\n**Recommended Fix:**\n<approach to fix, not full implementation>\n```\n\n**Status after investigation:**\n- Completed = You identified root cause and documented findings\n- Blocked = You cannot reproduce or need external resources\n- Uncertain = Multiple possible causes, need guidance on which to pursue\n```\n\n**Test:** Create an investigation task in Trace, assign to Executor, verify it produces findings rather than trying to write code.", "source": "engineer", "created_at": "2026-01-19T15:09:12.272837Z"}]}
{"id": "ralph-7ctr57", "title": "Executor: Clarify completion means verified", "description": "Executor prompt should clarify: work is 'Completed' only when verified by running tests. If tests can't be run (missing credentials, too slow, etc.), status is 'Blocked' not 'Completed.' Executors should not punt verification to the Verifier. The pattern 'I made a fix but needs verification' should not result in 'Completed' status.", "status": "closed", "priority": 2, "created_at": "2026-01-18T16:03:37.308055Z", "updated_at": "2026-01-19T15:15:35.316708Z", "closed_at": "2026-01-19T15:15:35.316708Z", "dependencies": [{"depends_on_id": "ralph-1c9p4g", "type": "parent"}], "comments": [{"content": "## Code Analysis\n\n**File to modify:**\n- `src/ralph2/agents/executor.py` - Strengthen completion requirements in system prompt\n\n**Current state:**\n- EXECUTOR_SYSTEM_PROMPT line 150-154 defines exit conditions:\n  - \"**Completed**: Work finished, tests pass, changes committed, traces updated\"\n- But this isn't strongly enforced - executors sometimes claim Completed without running tests\n- Lines 99-114 discuss verification boundaries but focus on capability vs behavior distinction\n\n**Implementation steps:**\n\n1. Add explicit section to `EXECUTOR_SYSTEM_PROMPT` after \"Test-Driven Development\" (~line 91):\n\n```python\n## CRITICAL: What \"Completed\" Means\n\n**Completed = Verified by Tests**\n\nYou may ONLY report status \"Completed\" when:\n1. You have written tests for the work (if applicable)\n2. You have RUN the tests (`uv run pytest` or equivalent)\n3. The tests PASS\n\n**If you cannot run tests:**\n- Tests require external resources \u2192 Status: **Blocked**, not Completed\n- Tests are too slow \u2192 Status: **Blocked**, not Completed  \n- No test framework exists \u2192 Status: **Blocked**, not Completed\n\n**The pattern \"I made a fix, needs verification\" = Blocked, NOT Completed**\n\nNever punt verification to the Verifier. The Verifier checks spec satisfaction, not your work quality. You own verification of your own work.\n```\n\n2. Update the \"Valid Exit Conditions\" section (lines 149-154) to reinforce:\n\n```python\n## Valid Exit Conditions\n\n- **Completed**: Tests written, tests run, tests PASS, changes committed\n- **Blocked**: Can't proceed OR can't verify (missing deps, external resources)\n- **Uncertain**: Not sure if approach is correct, need guidance\n```\n\n**Test:** Assign a code task, verify Executor runs tests before claiming Completed.", "source": "engineer", "created_at": "2026-01-19T15:09:10.714046Z"}]}
{"id": "ralph-7uw2af", "title": "src/ralph2/agents/executor.py:629-636: ClaudeAgentOptions construction uses options.tools instead of options.allowed_tools. In _run_executor_with_orchestrator_worktree(), the code references options.tools which may not be the correct attribute name. This could lead to runtime errors if the SDK interface changes.. Verify the correct attribute name is 'allowed_tools' (as used elsewhere) and ensure consistency across all ClaudeAgentOptions constructions.", "description": "Feedback from code_reviewer:\n\nsrc/ralph2/agents/executor.py:629-636: ClaudeAgentOptions construction uses options.tools instead of options.allowed_tools. In _run_executor_with_orchestrator_worktree(), the code references options.tools which may not be the correct attribute name. This could lead to runtime errors if the SDK interface changes.. Verify the correct attribute name is 'allowed_tools' (as used elsewhere) and ensure consistency across all ClaudeAgentOptions constructions.", "status": "open", "priority": 3, "created_at": "2026-01-20T17:53:26.997126Z", "updated_at": "2026-01-20T17:53:26.997126Z", "closed_at": null, "dependencies": [{"depends_on_id": "ralph-1pqntk", "type": "parent"}], "comments": []}
{"id": "ralph-bxwqor", "title": "src/ralph2/milestone.py:84-133: Function _categorize_work_items() has hardcoded keyword lists that may not cover all cases. The categorization logic uses static keyword lists ('feature', 'bug', 'refactor', 'docs') that may miss legitimate categories or miscategorize items. The 'Backlog' catch-all category hides classification failures.. Consider making keywords configurable or add a smarter heuristic. At minimum, add logging when items fall to Backlog so categorization gaps can be identified.", "description": "Feedback from code_reviewer:\n\nsrc/ralph2/milestone.py:84-133: Function _categorize_work_items() has hardcoded keyword lists that may not cover all cases. The categorization logic uses static keyword lists ('feature', 'bug', 'refactor', 'docs') that may miss legitimate categories or miscategorize items. The 'Backlog' catch-all category hides classification failures.. Consider making keywords configurable or add a smarter heuristic. At minimum, add logging when items fall to Backlog so categorization gaps can be identified.", "status": "open", "priority": 2, "created_at": "2026-01-20T17:53:26.891775Z", "updated_at": "2026-01-20T17:53:26.891775Z", "closed_at": null, "dependencies": [{"depends_on_id": "ralph-1pqntk", "type": "parent"}], "comments": []}
{"id": "ralph-dyjo23", "title": "Planner: Add iteration history visibility", "description": "Planner currently only sees last iteration's feedback. Should see summary of ALL previous iterations to recognize patterns like 'same test has failed 3 times with similar errors.' This enables the Planner to say 'stop trying fixes, investigate root cause first.' Implementation: Pass iteration history (number, intent, outcome, executor summary) to Planner prompt.", "status": "closed", "priority": 2, "created_at": "2026-01-18T16:03:27.564996Z", "updated_at": "2026-01-19T15:15:35.315472Z", "closed_at": "2026-01-19T15:15:35.315472Z", "dependencies": [{"depends_on_id": "ralph-1c9p4g", "type": "parent"}], "comments": [{"content": "## Code Analysis\n\n**Files to modify:**\n- `src/ralph2/runner.py` - Build iteration history before calling planner\n- `src/ralph2/agents/planner.py` - Add iteration_history parameter and update prompt\n\n**Current state:**\n- Planner only receives last iteration's feedback via `last_executor_summary`, `last_verifier_assessment`, `last_specialist_feedback` (runner.py:820-828)\n- No visibility into patterns across multiple iterations\n\n**Implementation steps:**\n\n1. In `runner.py`, add helper method to build iteration history:\n```python\ndef _build_iteration_history(self, run_id: str, current_iteration: int) -> list[dict]:\n    \"\"\"Build summary of previous iterations for pattern recognition.\"\"\"\n    iterations = self.db.list_iterations(run_id)\n    history = []\n    for it in iterations:\n        if it.number < current_iteration:\n            history.append({\n                \"number\": it.number,\n                \"intent\": it.intent,\n                \"outcome\": it.outcome,\n                # Get executor summary from agent outputs\n            })\n    return history\n```\n\n2. In `run_planner()` signature (planner.py:202), add parameter:\n```python\niteration_history: Optional[list[dict]] = None,\n```\n\n3. In planner prompt building (planner.py:229-316), add iteration history section:\n```python\nif iteration_history:\n    prompt_parts.append(\"# Iteration History\")\n    prompt_parts.append(\"\")\n    for h in iteration_history:\n        prompt_parts.append(f\"## Iteration {h['number']}\")\n        prompt_parts.append(f\"Intent: {h['intent']}\")\n        prompt_parts.append(f\"Outcome: {h['outcome']}\")\n        prompt_parts.append(\"\")\n```\n\n4. In `PLANNER_SYSTEM_PROMPT`, add guidance:\n```\n## Pattern Recognition\n\nReview iteration history to identify recurring patterns:\n- Same test failing multiple iterations \u2192 investigate root cause before more fixes\n- Same criterion stuck as unverifiable \u2192 may need different approach\n- Repeated \"Blocked\" from executors \u2192 investigate blocker systematically\n```\n\n**Test:** Run ralph2 for 3+ iterations and verify Planner prompt includes history.", "source": "engineer", "created_at": "2026-01-19T15:09:07.922298Z"}]}
{"id": "ralph-gqohqn", "title": "src/ralph2/state/db.py:363-378: Iteration conversion logic repeated in list_iterations() and get_latest_iteration(). Similar to the _row_to_run() refactoring that was done for Run objects, Iteration row-to-object conversion is duplicated in list_iterations() and get_latest_iteration() methods.. Extract a _row_to_iteration() helper method similar to _row_to_run() to DRY up the code and ensure consistent parsing.", "description": "Feedback from code_reviewer:\n\nsrc/ralph2/state/db.py:363-378: Iteration conversion logic repeated in list_iterations() and get_latest_iteration(). Similar to the _row_to_run() refactoring that was done for Run objects, Iteration row-to-object conversion is duplicated in list_iterations() and get_latest_iteration() methods.. Extract a _row_to_iteration() helper method similar to _row_to_run() to DRY up the code and ensure consistent parsing.", "status": "open", "priority": 3, "created_at": "2026-01-20T17:53:26.945066Z", "updated_at": "2026-01-20T17:53:26.945066Z", "closed_at": null, "dependencies": [{"depends_on_id": "ralph-1pqntk", "type": "parent"}], "comments": []}
{"id": "ralph-i5rcqp", "title": "Capture agent step patterns for meta-analysis", "description": "Enhance logging to capture structured data about what agents do when they succeed, enabling identification of meta-patterns that could inform future architecture decisions.\n\n## Context\nCurrent 3-agent structure (Planner/Executor/Verifier) works, but we want to understand:\n- What steps do agents consistently take when runs converge successfully?\n- Are there repeated sequences/patterns across successful runs?\n- What's the right granularity for future iterations of Ralph?\n\n## The insight\nRalph's principle (fresh context, focused task, loop until convergence) could potentially apply at finer granularity than 3 agents. But we need data to know where the natural boundaries are.\n\n## What to capture\n1. **Step sequences** - What each agent actually did, in order\n2. **Outcomes** - Did iteration make progress? Did run converge? How many iterations?\n3. **Correlation** - Which patterns appear in successful vs unsuccessful runs\n\n## Potential approaches\n- Add structured `steps_taken` field to agent outputs with defined vocabulary\n- Improve existing JSONL logging to be more analyzable\n- Build tooling to query patterns across runs\n\n## Goal\nCollect enough data to answer: \"Here's everything agents do consistently when we get a good result.\"", "status": "open", "priority": 2, "created_at": "2026-01-19T15:03:27.547254Z", "updated_at": "2026-01-19T15:03:27.547254Z", "closed_at": null, "dependencies": [], "comments": []}
{"id": "ralph-qmp15f", "title": "Add comprehensive tests for all agent patterns", "description": "Create test suite covering all agent patterns and infrastructure.\n\n## Test categories\n1. Output capture tests\n   - Capture creates JSONL file\n   - Includes timestamp, agent_type, prompt_summary\n   - Non-blocking on failure\n\n2. Error handling tests\n   - Transient errors trigger retries\n   - Fatal errors halt immediately\n   - Exponential backoff timing\n   - Max retries respected\n\n3. Structured output tests\n   - Valid output returns typed model\n   - Invalid output raises detailed error\n   - Various schema types (simple, nested, optional fields)\n\n4. Narrow agent tests\n   - Returns structured output\n   - Tool restriction works\n   - Conversation captured\n\n5. Walked agent tests\n   - Context persists across sends\n   - Each response captured\n   - Full transcript on end\n\n6. Bookended agent tests\n   - Setup executes first\n   - Work has setup context\n   - Wrap-up has full context\n   - All captured to JSONL\n\n## Technical\n- Use pytest\n- Mock Claude Agent SDK for unit tests\n- Consider integration tests with real API (marked slow)", "status": "closed", "priority": 2, "created_at": "2026-01-20T17:48:26.262460Z", "updated_at": "2026-01-20T18:01:34.532633Z", "closed_at": "2026-01-20T18:01:34.532633Z", "dependencies": [{"depends_on_id": "ralph-1tbdk4", "type": "parent"}], "comments": []}
{"id": "ralph-usxwlr", "title": "Verifier: Narrow scope to spec + test adequacy", "description": "Verifier's job is narrowly: (1) Is spec satisfied? (2) Are tests adequate to prove it? Verifier should NOT do root cause analysis on failures. Should NOT research why tests break. CAN say 'these tests are not good enough to have confidence spec is satisfied.' Focus: spec criteria + test quality + thumbs up/down. Remove efficiency notes from Verifier output - that's Executor's job.", "status": "closed", "priority": 2, "created_at": "2026-01-18T16:03:42.388358Z", "updated_at": "2026-01-19T15:10:32.825802Z", "closed_at": "2026-01-19T15:10:32.825802Z", "dependencies": [{"depends_on_id": "ralph-1c9p4g", "type": "parent"}], "comments": [{"content": "## Code Analysis\n\n**File to modify:**\n- `src/ralph2/agents/verifier.py` - Review and potentially remove efficiency notes\n\n**Current state:**\n- Verifier is already assessor-only (good) - see lines 15-20: \"You are an ASSESSOR, not a decision-maker\"\n- Verifier correctly does NOT decide CONTINUE/DONE/STUCK (lines 74-78)\n- BUT Verifier still has efficiency_notes section (lines 82-96) and outputs them\n\n**Issue description says:** \"Remove efficiency notes from Verifier output - that's Executor's job\"\n\n**Decision point:** This is debatable. Current efficiency notes are verification-specific:\n- \"Run `uv run pytest -v` to check all test criteria at once\"\n- \"CLI criteria can be verified with `--help` flags\"\n\nThese ARE useful for future Verifiers. But the issue says Executor owns efficiency notes.\n\n**If we proceed with removal:**\n\n1. Remove efficiency notes section from `VERIFIER_SYSTEM_PROMPT` (lines 82-96)\n\n2. Remove `efficiency_notes` field from `VerifierResult` model in `models.py` (line 120-123)\n\n3. Update `run_verifier()` to not include efficiency_notes in assessment output\n\n**Alternative (if we keep Verifier efficiency notes):**\n- Close this issue as \"won't fix\" with rationale\n- Verifier efficiency notes are about verification shortcuts, distinct from Executor efficiency notes about implementation shortcuts\n\n**Recommendation:** Ask product owner whether Verifier efficiency notes should be kept. They serve a different purpose than Executor notes.\n\n**What's definitely correct (already done):**\n- Verifier is assessor-only \u2713\n- Verifier does not decide CONTINUE/DONE/STUCK \u2713\n- Verifier does not recommend next steps \u2713", "source": "engineer", "created_at": "2026-01-19T15:09:11.925950Z"}, {"content": "## Decision: Keep Verifier efficiency notes\n\nEfficiency notes stay in Verifier. The purpose is to capture learnings from different perspectives:\n- Executor: implementation shortcuts (\"config is in settings.py\")\n- Verifier: verification shortcuts (\"run pytest -v to check all criteria\")\n\nBoth feed into project memory and help future iterations.\n\n## Revised Scope\n\nThe core requirement (Verifier is assessor-only) is **already implemented**:\n- \u2713 Verifier does NOT decide CONTINUE/DONE/STUCK (that's Planner's job)\n- \u2713 Verifier does NOT recommend next steps\n- \u2713 Verifier ONLY reports spec satisfaction status\n\n**No code changes needed.** This issue can be closed.\n\nVerify by reading `src/ralph2/agents/verifier.py` lines 15-20 and 74-78.", "source": "engineer", "created_at": "2026-01-19T15:10:28.261643Z"}]}
{"id": "ralph-vwwbxx", "title": "Implement narrow agent pattern", "description": "Create the narrow agent pattern - single prompt, structured output, optional tool restriction.\n\n## Requirements\n- Invoke with prompt and expected output schema\n- Return structured output matching schema\n- Support tool allowlist (agent only has access to listed tools)\n- Capture full conversation to JSONL file on completion\n\n## Interface\n```python\nclass NarrowAgent:\n    def invoke(\n        self, \n        prompt: str, \n        output_schema: Type[BaseModel],\n        tools: Optional[List[str]] = None\n    ) -> T:\n        \"\"\"Single invocation, returns structured output.\"\"\"\n```\n\n## Dependencies\n- Output capture module\n- Structured output validation\n- Error handling with retry\n\n## Verification\n- Returns Pydantic model matching output_schema\n- When tools specified, only those tools are available\n- Conversation captured to JSONL with timestamp, agent type, prompt summary", "status": "closed", "priority": 2, "created_at": "2026-01-20T17:48:15.267590Z", "updated_at": "2026-01-20T18:05:29.942795Z", "closed_at": "2026-01-20T18:05:29.942795Z", "dependencies": [{"depends_on_id": "ralph-1tbdk4", "type": "parent"}], "comments": [{"content": "Starting work on narrow agent pattern implementation. Will review existing soda infrastructure first.", "source": "executor", "created_at": "2026-01-20T18:01:56.224374Z"}, {"content": "Reviewed existing infrastructure: types.py, errors.py, validation.py, outputs/capture.py. Now writing tests first for NarrowAgent following TDD approach.", "source": "executor", "created_at": "2026-01-20T18:02:29.290356Z"}, {"content": "Implementation complete:\n- Created NarrowAgent class in src/soda/agents/narrow.py\n- Added async retry handler to errors.py (execute_with_retry_async)\n- 21 tests written and all passing\n- Full test suite (119 tests) passes\n- Changes committed to main branch\n\nImplementation details:\n- NarrowAgent.invoke() takes prompt, output_schema, optional tools, optional model\n- Uses existing StructuredOutputValidator for schema validation\n- Uses existing OutputCapture for JSONL conversation capture\n- Integrates with RetryHandler for error handling (transient vs fatal)\n- _call_agent method can be mocked for testing (Claude SDK call)", "source": "executor", "created_at": "2026-01-20T18:05:27.055486Z"}]}
